{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d3f7a",
   "metadata": {},
   "source": [
    "# GoingDeeper NLP 프로젝트: 멋진 단어사전 만들기\n",
    "\n",
    "- [ ] SentencePiece의 성능을 다각도로 비교분석하였는가?\t\n",
    "        - SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47034f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b6e17",
   "metadata": {},
   "source": [
    "### 네이버 영화리뷰 감정 분석 문제  \n",
    "  \n",
    "- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기  \n",
    "- SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0860278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab, Okt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f6ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(file_path):\n",
    "    \"\"\"\n",
    "    데이터 파일을 로드하고 결측값을 제거한 후 기본적인 EDA 수행합니다.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\").dropna()\n",
    "    print(\"데이터 개수:\", len(df))\n",
    "    print(\"라벨 분포:\")\n",
    "    print(df['label'].value_counts())\n",
    "    df['length'] = df['document'].apply(lambda x: len(str(x)))\n",
    "    print(\"문장 길이 통계:\")\n",
    "    print(df['length'].describe(),'\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3307ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 149995\n",
      "라벨 분포:\n",
      "0    75170\n",
      "1    74825\n",
      "Name: label, dtype: int64\n",
      "문장 길이 통계:\n",
      "count    149995.000000\n",
      "mean         35.204527\n",
      "std          29.531890\n",
      "min           1.000000\n",
      "25%          16.000000\n",
      "50%          27.000000\n",
      "75%          42.000000\n",
      "max         146.000000\n",
      "Name: length, dtype: float64 \n",
      "\n",
      "데이터 개수: 49997\n",
      "라벨 분포:\n",
      "1    25171\n",
      "0    24826\n",
      "Name: label, dtype: int64\n",
      "문장 길이 통계:\n",
      "count    49997.000000\n",
      "mean        35.320259\n",
      "std         29.648310\n",
      "min          1.000000\n",
      "25%         16.000000\n",
      "50%         27.000000\n",
      "75%         43.000000\n",
      "max        144.000000\n",
      "Name: length, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = eda(\"./data/ratings_train.txt\")\n",
    "df_test = eda(\"./data/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bd4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def preprocess_text(df):\n",
    "    df['document'] = df['document'].astype(str).str.strip()\n",
    "    df['document'] = df['document'].str.replace(\"[^가-힣0-9a-zA-Z\\s]\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_text(df_train)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27edff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['document']).reset_index(drop=True)\n",
    "df_test = df_test.drop_duplicates(subset=['document']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a255b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143899\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label  length\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0      19\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1      33\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0      17\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0      29\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1      61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb45429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데 왜 3D로 나와서 제 심기를 불편하게 하죠</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       document  label  length\n",
       "0  6270596                                             굳       1       3\n",
       "1  9274899                           GDNTOPCLASSINTHECLUB      0      20\n",
       "2  8544678             뭐야 이 평점들은 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0      38\n",
       "3  6825595                      지루하지는 않은데 완전 막장임 돈주고 보기에는      0      32\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데 왜 3D로 나와서 제 심기를 불편하게 하죠      0      49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6292f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 모델 학습을 위한 데이터 준비\n",
    "def prepare_sentencepiece_data(df, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for text in df['document']:\n",
    "            if isinstance(text, str) and text.strip():  # 빈 문자열 제외\n",
    "                f.write(text + '\\n')\n",
    "prepare_sentencepiece_data(df_train, 'naver_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c01f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 토큰화 함수\n",
    "def sp_tokenize(sp, corpus, maxlen):\n",
    "    tensor = [sp.encode_as_ids(sen) for sen in corpus]\n",
    "    tensor = pad_sequences(tensor, maxlen=maxlen, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f50f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 모델 학습 및 평가\n",
    "def train_sentencepiece_model(model_type):\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input=naver_corpus.txt --model_prefix=sp_model5000 --vocab_size=5000 '\n",
    "        f'--model_type={model_type} --character_coverage=1.0 --minloglevel=0'\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('sp_model.model')\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7339c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mecab(corpus):\n",
    "    mecab = Mecab()\n",
    "    return [mecab.morphs(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c10a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_okt(corpus):\n",
    "    okt = Okt()\n",
    "    return [okt.morphs(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c90917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 패딩 적용\n",
    "def tokenizer_and_pad(tokenized_corpus, maxlen):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "    tensor = pad_sequences(tensor, maxlen=maxlen, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e0b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 데이터 정의\n",
    "y = np.array(df_train['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722ced4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(5000, 256, input_length=X.shape[1]),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83810513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험 수행 및 비교\n",
    "results = {}\n",
    "epochs_completed = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10bd7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=naver_corpus.txt --model_prefix=sp_model5000 --vocab_size=5000 --model_type=bpe --character_coverage=1.0 --minloglevel=0\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: naver_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_model5000\n",
      "  model_type: BPE\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: naver_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 143894 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4945989\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=2558\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 143894 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 143894\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 303182\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=58653 min_freq=79\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10576 size=20 all=117893 active=11359 piece=▁내\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8114 size=40 all=122653 active=16119 piece=에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5713 size=60 all=126564 active=20030 piece=▁더\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4753 size=80 all=131667 active=25133 piece=▁같\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3987 size=100 all=135284 active=28750 piece=보다\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3944 min_freq=66\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3546 size=120 all=138407 active=9373 piece=▁뭐\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2989 size=140 all=141047 active=12013 piece=▁참\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2719 size=160 all=143441 active=14407 piece=▁상\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2462 size=180 all=146126 active=17092 piece=지는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2279 size=200 all=148634 active=19600 piece=▁코\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2278 min_freq=59\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2100 size=220 all=151397 active=10102 piece=했다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1933 size=240 all=154806 active=13511 piece=▁허\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1811 size=260 all=157547 active=16252 piece=▁로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1725 size=280 all=159597 active=18302 piece=▁재밌게\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1586 size=300 all=161731 active=20436 piece=▁엄\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1586 min_freq=53\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1455 size=320 all=163810 active=10124 piece=▁단\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1399 size=340 all=165499 active=11813 piece=아서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1322 size=360 all=168276 active=14590 piece=였다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1246 size=380 all=170850 active=17164 piece=▁OO\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1209 size=400 all=173024 active=19338 piece=하지만\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1207 min_freq=48\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1156 size=420 all=175019 active=10346 piece=려고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1096 size=440 all=177315 active=12642 piece=▁극장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1057 size=460 all=178982 active=14309 piece=▁억지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=998 size=480 all=181129 active=16456 piece=▁전혀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=956 size=500 all=182644 active=17971 piece=했던\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=954 min_freq=45\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=917 size=520 all=184695 active=10916 piece=▁재밋\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=886 size=540 all=186692 active=12913 piece=▁크\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=863 size=560 all=189457 active=15678 piece=▁이걸\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=840 size=580 all=191453 active=17674 piece=영화는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=810 size=600 all=194042 active=20263 piece=▁모습\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=810 min_freq=41\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=778 size=620 all=195677 active=11293 piece=보면\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=763 size=640 all=198178 active=13794 piece=▁느껴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=745 size=660 all=200349 active=15965 piece=들도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=718 size=680 all=202060 active=17676 piece=▁내용도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=693 size=700 all=203696 a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 23s 9ms/step - loss: 0.4529 - accuracy: 0.7672 - val_loss: 0.3758 - val_accuracy: 0.8240\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.3425 - accuracy: 0.8507 - val_loss: 0.3739 - val_accuracy: 0.8392\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.3084 - accuracy: 0.8671 - val_loss: 0.3534 - val_accuracy: 0.8523\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2809 - accuracy: 0.8808 - val_loss: 0.4186 - val_accuracy: 0.8500\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2540 - accuracy: 0.8940 - val_loss: 0.3408 - val_accuracy: 0.8510\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2266 - accuracy: 0.9073 - val_loss: 0.4465 - val_accuracy: 0.8470\n",
      "Epoch 7/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2003 - accuracy: 0.9199 - val_loss: 0.4090 - val_accuracy: 0.8475\n",
      "Epoch 8/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.1751 - accuracy: 0.9315 - val_loss: 0.4403 - val_accuracy: 0.8436\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2469 - accuracy: 0.9095\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 21s 9ms/step - loss: 0.4581 - accuracy: 0.7645 - val_loss: 0.3720 - val_accuracy: 0.8279\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.3464 - accuracy: 0.8482 - val_loss: 0.3508 - val_accuracy: 0.8471\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.3101 - accuracy: 0.8647 - val_loss: 0.3523 - val_accuracy: 0.8531\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2811 - accuracy: 0.8787 - val_loss: 0.3530 - val_accuracy: 0.8521\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2533 - accuracy: 0.8924 - val_loss: 0.3985 - val_accuracy: 0.8217\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.3166 - accuracy: 0.8719\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 9ms/step - loss: 0.4296 - accuracy: 0.7915 - val_loss: 0.4028 - val_accuracy: 0.8259\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3437 - accuracy: 0.8483 - val_loss: 0.3491 - val_accuracy: 0.8448\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 10ms/step - loss: 0.3083 - accuracy: 0.8665 - val_loss: 0.4039 - val_accuracy: 0.8562\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2810 - accuracy: 0.8806 - val_loss: 0.3423 - val_accuracy: 0.8489\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2549 - accuracy: 0.8928 - val_loss: 0.4051 - val_accuracy: 0.8542\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2288 - accuracy: 0.9055 - val_loss: 0.3789 - val_accuracy: 0.8504\n",
      "Epoch 7/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2036 - accuracy: 0.9180 - val_loss: 0.3603 - val_accuracy: 0.8484\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2718 - accuracy: 0.8964\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 9ms/step - loss: 0.4543 - accuracy: 0.7670 - val_loss: 0.4015 - val_accuracy: 0.8340\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3403 - accuracy: 0.8501 - val_loss: 0.3458 - val_accuracy: 0.8507\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.3069 - accuracy: 0.8670 - val_loss: 0.3526 - val_accuracy: 0.8512\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 18s 9ms/step - loss: 0.2789 - accuracy: 0.8815 - val_loss: 0.3725 - val_accuracy: 0.8545\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2526 - accuracy: 0.8938 - val_loss: 0.3423 - val_accuracy: 0.8519\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2256 - accuracy: 0.9070 - val_loss: 0.3862 - val_accuracy: 0.8480\n",
      "Epoch 7/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2008 - accuracy: 0.9189 - val_loss: 0.4752 - val_accuracy: 0.8429\n",
      "Epoch 8/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.1770 - accuracy: 0.9303 - val_loss: 0.3981 - val_accuracy: 0.8420\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2285 - accuracy: 0.9162\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 비교 (bpe, unigram, word, char)\n",
    "for model_type in ['bpe', 'unigram', 'word', 'char']:\n",
    "    sp_model = train_sentencepiece_model(model_type)\n",
    "    X_sp = sp_tokenize(sp_model, df_train['document'].tolist(), 65)\n",
    "    model, history = train_and_evaluate(X_sp, y)\n",
    "    results[f'SentencePiece-{model_type}'] = model.evaluate(X_sp, y)\n",
    "    epochs_completed[f'SentencePiece-{model_type}'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cedea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 23s 10ms/step - loss: 0.4289 - accuracy: 0.7878 - val_loss: 0.3509 - val_accuracy: 0.8466\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3271 - accuracy: 0.8579 - val_loss: 0.3221 - val_accuracy: 0.8598\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2943 - accuracy: 0.8740 - val_loss: 0.3448 - val_accuracy: 0.8384\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2696 - accuracy: 0.8851 - val_loss: 0.3394 - val_accuracy: 0.8633\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2456 - accuracy: 0.8964 - val_loss: 0.4528 - val_accuracy: 0.8404\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2905 - accuracy: 0.8763\n"
     ]
    }
   ],
   "source": [
    "# Mecab 비교\n",
    "mecab_tokens = tokenize_mecab(df_train['document'].tolist())\n",
    "X_mecab = tokenizer_and_pad(mecab_tokens, 65)\n",
    "model, history = train_and_evaluate(X_mecab, y)\n",
    "results['Mecab'] = model.evaluate(X_mecab, y)\n",
    "epochs_completed['Mecab'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f9e9298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 9ms/step - loss: 0.4444 - accuracy: 0.7756 - val_loss: 0.4323 - val_accuracy: 0.7833\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3604 - accuracy: 0.8360 - val_loss: 0.3525 - val_accuracy: 0.8340\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3312 - accuracy: 0.8490 - val_loss: 0.3952 - val_accuracy: 0.8318\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3076 - accuracy: 0.8597 - val_loss: 0.3521 - val_accuracy: 0.8381\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2879 - accuracy: 0.8693 - val_loss: 0.3698 - val_accuracy: 0.8306\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2671 - accuracy: 0.8796 - val_loss: 0.3742 - val_accuracy: 0.8345\n",
      "Epoch 7/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2465 - accuracy: 0.8894 - val_loss: 0.3840 - val_accuracy: 0.8265\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2788 - accuracy: 0.8722\n"
     ]
    }
   ],
   "source": [
    "# Okt 비교\n",
    "okt_tokens = tokenize_okt(df_train['document'].tolist())\n",
    "X_okt = tokenizer_and_pad(okt_tokens, 65)\n",
    "model, history = train_and_evaluate(X_okt, y)\n",
    "results['Okt'] = model.evaluate(X_okt, y)\n",
    "epochs_completed['Okt'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9807a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece-bpe: Loss=0.2469, Accuracy=0.9095, Epochs Completed=8\n",
      "SentencePiece-unigram: Loss=0.3166, Accuracy=0.8719, Epochs Completed=5\n",
      "SentencePiece-word: Loss=0.2718, Accuracy=0.8964, Epochs Completed=7\n",
      "SentencePiece-char: Loss=0.2285, Accuracy=0.9162, Epochs Completed=8\n",
      "Mecab: Loss=0.2905, Accuracy=0.8763, Epochs Completed=5\n",
      "Okt: Loss=0.2788, Accuracy=0.8722, Epochs Completed=7\n"
     ]
    }
   ],
   "source": [
    "# 비교 결과 출력\n",
    "for method, (loss, accuracy) in results.items():\n",
    "    print(f\"{method}: Loss={loss:.4f}, Accuracy={accuracy:.4f}, Epochs Completed={epochs_completed[method]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a8579",
   "metadata": {},
   "source": [
    "## 정리\n",
    "| Tokenizer           | Loss  | Accuracy | Epochs |\n",
    "|--------------------|-------|----------|------------------|\n",
    "| SentencePiece-bpe  | 0.2469 | 0.9095   | 8                |\n",
    "| SentencePiece-unigram | 0.3166 | 0.8719   | 5                |\n",
    "| SentencePiece-word | 0.2718 | 0.8964   | 7                |\n",
    "| SentencePiece-char | 0.2285 | 0.9162   | 8                |\n",
    "| Mecab             | 0.2905 | 0.8763   | 5                |\n",
    "| Okt               | 0.2788 | 0.8722   | 7                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87402f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
