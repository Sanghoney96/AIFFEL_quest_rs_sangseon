{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f17dfd",
   "metadata": {},
   "source": [
    "# GoingDeeper NLP 프로젝트: 멋진 단어사전 만들기\n",
    "\n",
    "- vocab size = 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5b2f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999385f3",
   "metadata": {},
   "source": [
    "### 네이버 영화리뷰 감정 분석 문제  \n",
    "  \n",
    "- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기  \n",
    "- SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ad4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab, Okt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c580f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\").dropna()\n",
    "    print(\"데이터 개수:\", len(df))\n",
    "    print(\"라벨 분포:\")\n",
    "    print(df['label'].value_counts())\n",
    "    df['length'] = df['document'].apply(lambda x: len(str(x)))\n",
    "    print(\"문장 길이 통계:\")\n",
    "    print(df['length'].describe(),'\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c50d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 149995\n",
      "라벨 분포:\n",
      "0    75170\n",
      "1    74825\n",
      "Name: label, dtype: int64\n",
      "문장 길이 통계:\n",
      "count    149995.000000\n",
      "mean         35.204527\n",
      "std          29.531890\n",
      "min           1.000000\n",
      "25%          16.000000\n",
      "50%          27.000000\n",
      "75%          42.000000\n",
      "max         146.000000\n",
      "Name: length, dtype: float64 \n",
      "\n",
      "데이터 개수: 49997\n",
      "라벨 분포:\n",
      "1    25171\n",
      "0    24826\n",
      "Name: label, dtype: int64\n",
      "문장 길이 통계:\n",
      "count    49997.000000\n",
      "mean        35.320259\n",
      "std         29.648310\n",
      "min          1.000000\n",
      "25%         16.000000\n",
      "50%         27.000000\n",
      "75%         43.000000\n",
      "max        144.000000\n",
      "Name: length, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = eda(\"./data/ratings_train.txt\")\n",
    "df_test = eda(\"./data/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc66f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def preprocess_text(df):\n",
    "    df['document'] = df['document'].astype(str).str.strip()\n",
    "    df['document'] = df['document'].str.replace(\"[^가-힣0-9a-zA-Z\\s]\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_text(df_train)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab152ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['document']).reset_index(drop=True)\n",
    "df_test = df_test.drop_duplicates(subset=['document']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d24b67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143899\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label  length\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0      19\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1      33\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0      17\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0      29\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1      61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1932fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데 왜 3D로 나와서 제 심기를 불편하게 하죠</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       document  label  length\n",
       "0  6270596                                             굳       1       3\n",
       "1  9274899                           GDNTOPCLASSINTHECLUB      0      20\n",
       "2  8544678             뭐야 이 평점들은 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0      38\n",
       "3  6825595                      지루하지는 않은데 완전 막장임 돈주고 보기에는      0      32\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데 왜 3D로 나와서 제 심기를 불편하게 하죠      0      49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc14f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 모델 학습을 위한 데이터 준비\n",
    "def prepare_sentencepiece_data(df, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for text in df['document']:\n",
    "            if isinstance(text, str) and text.strip():  # 빈 문자열 제외\n",
    "                f.write(text + '\\n')\n",
    "prepare_sentencepiece_data(df_train, 'naver_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f4cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 토큰화 함수\n",
    "def sp_tokenize(sp, corpus, maxlen):\n",
    "    tensor = [sp.encode_as_ids(sen) for sen in corpus]\n",
    "    tensor = pad_sequences(tensor, maxlen=maxlen, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe8b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 모델 학습 및 평가\n",
    "def train_sentencepiece_model(model_type):\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input=naver_corpus.txt --model_prefix=sp_model5000 --vocab_size=5000 '\n",
    "        f'--model_type={model_type} --character_coverage=1.0 --minloglevel=0'\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('sp_model.model')\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f2ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mecab(corpus):\n",
    "    mecab = Mecab()\n",
    "    return [mecab.morphs(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00000a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_okt(corpus):\n",
    "    okt = Okt()\n",
    "    return [okt.morphs(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e51973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 패딩 적용\n",
    "def tokenizer_and_pad(tokenized_corpus, maxlen):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "    tensor = pad_sequences(tensor, maxlen=maxlen, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3bc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 데이터 정의\n",
    "y = np.array(df_train['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2426caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(8000, 256, input_length=X.shape[1]),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253b8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험 수행 및 비교\n",
    "results = {}\n",
    "epochs_completed = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561bbede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=naver_corpus.txt --model_prefix=sp_model5000 --vocab_size=5000 --model_type=bpe --character_coverage=1.0 --minloglevel=0\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: naver_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_model5000\n",
      "  model_type: BPE\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: naver_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 143894 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4945989\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=2558\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 143894 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 143894\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 303182\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=58653 min_freq=79\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10576 size=20 all=117893 active=11359 piece=▁내\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8114 size=40 all=122653 active=16119 piece=에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5713 size=60 all=126564 active=20030 piece=▁더\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4753 size=80 all=131667 active=25133 piece=▁같\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3987 size=100 all=135284 active=28750 piece=보다\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3944 min_freq=66\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3546 size=120 all=138407 active=9373 piece=▁뭐\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2989 size=140 all=141047 active=12013 piece=▁참\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2719 size=160 all=143441 active=14407 piece=▁상\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2462 size=180 all=146126 active=17092 piece=지는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2279 size=200 all=148634 active=19600 piece=▁코\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2278 min_freq=59\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2100 size=220 all=151397 active=10102 piece=했다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1933 size=240 all=154806 active=13511 piece=▁허\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1811 size=260 all=157547 active=16252 piece=▁로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1725 size=280 all=159597 active=18302 piece=▁재밌게\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1586 size=300 all=161731 active=20436 piece=▁엄\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1586 min_freq=53\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1455 size=320 all=163810 active=10124 piece=▁단\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1399 size=340 all=165499 active=11813 piece=아서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1322 size=360 all=168276 active=14590 piece=였다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1246 size=380 all=170850 active=17164 piece=▁OO\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1209 size=400 all=173024 active=19338 piece=하지만\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1207 min_freq=48\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1156 size=420 all=175019 active=10346 piece=려고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1096 size=440 all=177315 active=12642 piece=▁극장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1057 size=460 all=178982 active=14309 piece=▁억지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=998 size=480 all=181129 active=16456 piece=▁전혀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=956 size=500 all=182644 active=17971 piece=했던\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=954 min_freq=45\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=917 size=520 all=184695 active=10916 piece=▁재밋\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=886 size=540 all=186692 active=12913 piece=▁크\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=863 size=560 all=189457 active=15678 piece=▁이걸\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=840 size=580 all=191453 active=17674 piece=영화는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=810 size=600 all=194042 active=20263 piece=▁모습\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=810 min_freq=41\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=778 size=620 all=195677 active=11293 piece=보면\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=763 size=640 all=198178 active=13794 piece=▁느껴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=745 size=660 all=200349 active=15965 piece=들도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=718 size=680 all=202060 active=17676 piece=▁내용도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=693 size=700 all=203696 active=19312 piece=▁날\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=693 min_freq=38\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=666 size=720 all=205199 active=11610 piece=▁싶다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=654 size=740 all=206584 active=12995 piece=▁남는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=637 size=760 all=208093 active=14504 piece=▁따뜻\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=624 size=780 all=209647 active=16058 piece=하는데\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=610 size=800 all=210868 active=17279 piece=▁희\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=610 min_freq=36\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=599 size=820 all=212539 active=12185 piece=▁언제\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=579 size=840 all=213744 active=13390 piece=▁마지막에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=564 size=860 all=214952 active=14598 piece=▁재미가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=553 size=880 all=216564 active=16210 piece=▁없이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=542 size=900 all=217991 active=17637 piece=▁흐\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=540 min_freq=35\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=531 size=920 all=219200 active=12069 piece=▁함께\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=521 size=940 all=220160 active=13029 piece=▁뛰\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=513 size=960 all=221176 active=14045 piece=▁포스터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=498 size=980 all=222684 active=15553 piece=▁맘\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=486 size=1000 all=223806 active=16675 piece=▁돌아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=486 min_freq=33\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=476 size=1020 all=225835 active=13189 piece=▁충격\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=465 size=1040 all=227849 active=15203 piece=▁낫\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=453 size=1060 all=228920 active=16274 piece=▁따라\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=440 size=1080 all=230080 active=17434 piece=▁괴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=431 size=1100 all=231318 active=18672 piece=수있\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=431 min_freq=32\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=422 size=1120 all=232636 active=12789 piece=▁너\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=414 size=1140 all=234168 active=14321 piece=▁훨씬\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=404 size=1160 all=235846 active=15999 piece=▁갈수록\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=395 size=1180 all=237140 act"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 24s 10ms/step - loss: 0.4693 - accuracy: 0.7486 - val_loss: 0.3643 - val_accuracy: 0.8380\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3400 - accuracy: 0.8514 - val_loss: 0.3338 - val_accuracy: 0.8493\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3063 - accuracy: 0.8672 - val_loss: 0.3606 - val_accuracy: 0.8539\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2776 - accuracy: 0.8815 - val_loss: 0.3569 - val_accuracy: 0.8552\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2516 - accuracy: 0.8947 - val_loss: 0.3429 - val_accuracy: 0.8470\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2940 - accuracy: 0.8749\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 10ms/step - loss: 0.4382 - accuracy: 0.7837 - val_loss: 0.3827 - val_accuracy: 0.8320\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3411 - accuracy: 0.8508 - val_loss: 0.3370 - val_accuracy: 0.8519\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3057 - accuracy: 0.8673 - val_loss: 0.3643 - val_accuracy: 0.8536\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2775 - accuracy: 0.8809 - val_loss: 0.3699 - val_accuracy: 0.8502\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2505 - accuracy: 0.8935 - val_loss: 0.3449 - val_accuracy: 0.8524\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2889 - accuracy: 0.8739\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 23s 10ms/step - loss: 0.4586 - accuracy: 0.7615 - val_loss: 0.3682 - val_accuracy: 0.8356\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3410 - accuracy: 0.8497 - val_loss: 0.3974 - val_accuracy: 0.8361\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3068 - accuracy: 0.8673 - val_loss: 0.3565 - val_accuracy: 0.8407\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2778 - accuracy: 0.8811 - val_loss: 0.4505 - val_accuracy: 0.8482\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2518 - accuracy: 0.8934 - val_loss: 0.3438 - val_accuracy: 0.8549\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2258 - accuracy: 0.9065 - val_loss: 0.3516 - val_accuracy: 0.8530\n",
      "Epoch 7/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2013 - accuracy: 0.9185 - val_loss: 0.3630 - val_accuracy: 0.8483\n",
      "Epoch 8/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.1766 - accuracy: 0.9296 - val_loss: 0.4384 - val_accuracy: 0.8468\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2538 - accuracy: 0.9131\n",
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 10ms/step - loss: 0.4584 - accuracy: 0.7630 - val_loss: 0.3814 - val_accuracy: 0.8384\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3446 - accuracy: 0.8495 - val_loss: 0.3286 - val_accuracy: 0.8530\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3077 - accuracy: 0.8672 - val_loss: 0.3415 - val_accuracy: 0.8513\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2801 - accuracy: 0.8801 - val_loss: 0.3474 - val_accuracy: 0.8509\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2540 - accuracy: 0.8927 - val_loss: 0.3832 - val_accuracy: 0.8535\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2862 - accuracy: 0.8767\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 비교 (bpe, unigram, word, char)\n",
    "for model_type in ['bpe', 'unigram', 'word', 'char']:\n",
    "    sp_model = train_sentencepiece_model(model_type)\n",
    "    X_sp = sp_tokenize(sp_model, df_train['document'].tolist(), 65)\n",
    "    model, history = train_and_evaluate(X_sp, y)\n",
    "    results[f'SentencePiece-{model_type}'] = model.evaluate(X_sp, y)\n",
    "    epochs_completed[f'SentencePiece-{model_type}'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16d6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 22s 10ms/step - loss: 0.4234 - accuracy: 0.7923 - val_loss: 0.3516 - val_accuracy: 0.8484\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3201 - accuracy: 0.8616 - val_loss: 0.3612 - val_accuracy: 0.8409\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2812 - accuracy: 0.8806 - val_loss: 0.3076 - val_accuracy: 0.8680\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2499 - accuracy: 0.8956 - val_loss: 0.3358 - val_accuracy: 0.8479\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2217 - accuracy: 0.9089 - val_loss: 0.4070 - val_accuracy: 0.8615\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.1961 - accuracy: 0.9210 - val_loss: 0.3757 - val_accuracy: 0.8605\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.2446 - accuracy: 0.9044\n"
     ]
    }
   ],
   "source": [
    "# Mecab 비교\n",
    "mecab_tokens = tokenize_mecab(df_train['document'].tolist())\n",
    "X_mecab = tokenizer_and_pad(mecab_tokens, 65)\n",
    "model, history = train_and_evaluate(X_mecab, y)\n",
    "results['Mecab'] = model.evaluate(X_mecab, y)\n",
    "epochs_completed['Mecab'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdb002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2024/2024 [==============================] - 23s 10ms/step - loss: 0.4383 - accuracy: 0.7799 - val_loss: 0.4357 - val_accuracy: 0.8281\n",
      "Epoch 2/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.3429 - accuracy: 0.8471 - val_loss: 0.4881 - val_accuracy: 0.8247\n",
      "Epoch 3/20\n",
      "2024/2024 [==============================] - 19s 10ms/step - loss: 0.3073 - accuracy: 0.8629 - val_loss: 0.3569 - val_accuracy: 0.8473\n",
      "Epoch 4/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2762 - accuracy: 0.8770 - val_loss: 0.3956 - val_accuracy: 0.8448\n",
      "Epoch 5/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2492 - accuracy: 0.8903 - val_loss: 0.6360 - val_accuracy: 0.6359\n",
      "Epoch 6/20\n",
      "2024/2024 [==============================] - 19s 9ms/step - loss: 0.2215 - accuracy: 0.9037 - val_loss: 0.4100 - val_accuracy: 0.8411\n",
      "4497/4497 [==============================] - 17s 4ms/step - loss: 0.3101 - accuracy: 0.8835\n"
     ]
    }
   ],
   "source": [
    "# Okt 비교\n",
    "okt_tokens = tokenize_okt(df_train['document'].tolist())\n",
    "X_okt = tokenizer_and_pad(okt_tokens, 65)\n",
    "model, history = train_and_evaluate(X_okt, y)\n",
    "results['Okt'] = model.evaluate(X_okt, y)\n",
    "epochs_completed['Okt'] = len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a926d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece-bpe: Loss=0.2940, Accuracy=0.8749, Epochs Completed=5\n",
      "SentencePiece-unigram: Loss=0.2889, Accuracy=0.8739, Epochs Completed=5\n",
      "SentencePiece-word: Loss=0.2538, Accuracy=0.9131, Epochs Completed=8\n",
      "SentencePiece-char: Loss=0.2862, Accuracy=0.8767, Epochs Completed=5\n",
      "Mecab: Loss=0.2446, Accuracy=0.9044, Epochs Completed=6\n",
      "Okt: Loss=0.3101, Accuracy=0.8835, Epochs Completed=6\n"
     ]
    }
   ],
   "source": [
    "# 비교 결과 출력\n",
    "for method, (loss, accuracy) in results.items():\n",
    "    print(f\"{method}: Loss={loss:.4f}, Accuracy={accuracy:.4f}, Epochs Completed={epochs_completed[method]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa8102",
   "metadata": {},
   "source": [
    "## 정리\n",
    "| Tokenizer           | Loss  | Accuracy | Epochs |\n",
    "|--------------------|-------|----------|------------------|\n",
    "| SentencePiece-bpe  | 0.2940 | 0.8749   | 5                |\n",
    "| SentencePiece-unigram | 0.2889 | 0.8739   | 5                |\n",
    "| SentencePiece-word | 0.2538 | 0.9131   | 8                |\n",
    "| SentencePiece-char | 0.2862 | 0.8767   | 5                |\n",
    "| Mecab             | 0.2446 | 0.9044   | 6                |\n",
    "| Okt               | 0.3101 | 0.8835   | 6                |\n",
    "\n",
    "\n",
    "\n",
    "- 그전 vocab 사이즈에 따라 accuary가 달라지는 것을 보아. 데이터의 크기가 크면 클수록 형태소 분석 토큰화가 더 성능이 좋아질 것을 보임.\n",
    "- 실험을 해봐야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f93c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
