{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3690fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "# 한국어를 지원하는 폰트로 변경\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b11cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7569b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축 해제 완료!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# 압축 풀기\n",
    "tar_path = \"./data/korean-english-park.train.tar.gz\"\n",
    "extract_path = \"./data/korean-english-park/\"\n",
    "\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "print(\"압축 해제 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67f761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean-english-park.train.en', 'korean-english-park.train.ko']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dad56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 변환 완료! 'korean.txt'와 'english.txt'가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "kor_file = \"./data/korean-english-park/korean-english-park.train.ko\"\n",
    "eng_file = \"./data/korean-english-park/korean-english-park.train.en\"\n",
    "\n",
    "# 변환된 파일 저장 경로\n",
    "kor_output = \"./data/korean.txt\"\n",
    "eng_output = \"./data/english.txt\"\n",
    "\n",
    "# 한국어, 영어 문장을 각각 저장\n",
    "with open(kor_file, \"r\", encoding=\"utf-8\") as f_ko, open(eng_file, \"r\", encoding=\"utf-8\") as f_en:\n",
    "    korean_sentences = f_ko.readlines()\n",
    "    english_sentences = f_en.readlines()\n",
    "\n",
    "# 새로운 파일로 저장\n",
    "with open(kor_output, \"w\", encoding=\"utf-8\") as f_ko, open(eng_output, \"w\", encoding=\"utf-8\") as f_en:\n",
    "    f_ko.writelines(korean_sentences)\n",
    "    f_en.writelines(english_sentences)\n",
    "\n",
    "print(\"파일 변환 완료! 'korean.txt'와 'english.txt'가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70203d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"특수 문자 및 불필요한 공백 제거\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", sentence)  # 한글과 공백만 남기기\n",
    "    else:  # 영어\n",
    "        sentence = sentence.lower().strip()  # 소문자로 변환 및 공백 제거\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]\", \" \", sentence)  # 특수 문자 제거\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 불필요한 공백 제거\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784096d5",
   "metadata": {},
   "source": [
    "# Step2 : 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180f6ca",
   "metadata": {},
   "source": [
    "### 정제하기 & 토큰화하기\n",
    "- 데이터는 \\t 기호를 기준으로 영어와 스페인어가 병렬 쌍\n",
    "- \\t 기호를 매개변수로 split() 함수를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62b9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "# MeCab 토크나이저 초기화\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"특수 문자 및 불필요한 공백 제거\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", sentence)  # 한글과 공백만 남기기\n",
    "    else:  # 영어\n",
    "        sentence = sentence.lower().strip()  # 소문자로 변환 및 공백 제거\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]\", \" \", sentence)  # 특수 문자 제거\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 불필요한 공백 제거\n",
    "    return sentence\n",
    "\n",
    "def tokenize_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"토큰화 수행\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        return mecab.parse(sentence).strip()  # MeCab 토큰화\n",
    "    else:  # 영어\n",
    "        return \"<start> \" + sentence + \" <end>\"  # 시작과 끝 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e440602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 정제 및 토큰화 완료!\n",
      "🔹 중복 제거 후 데이터 개수: 66658\n",
      "🔹 한국어 샘플: ['쿠바 서부 지역 의 토양 은 세계 에서 담배 를 기르 는 최적 의 장소 이 다', '남북한 은 년 역사 적 인 정상 회담 으로 남북 관계 가 해빙 되 기 전 수 십 년 동안 서로 를 비방 했 다', '매케인 은 우리 는 새롭 고 간단 한 세금 시스템 을 만들 어 미국인 들 에게 기회 를 주 려고 한다고 덧붙였 다']\n",
      "🔹 영어 샘플: ['<start> the soil in the western part of the country is considered the best in the world for growing tobacco. <end>', '<start> before their historic summit in thawed relations, the two koreas had vilified each other for decades. <end>', '<start> we are going to create a new and simpler tax system and give the american people a choice. <end>']\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "kor_path = \"./data/korean.txt\"\n",
    "eng_path = \"./data/english.txt\"\n",
    "\n",
    "# 중복 제거를 위한 set 활용\n",
    "unique_pairs = set()\n",
    "\n",
    "with open(kor_path, \"r\", encoding=\"utf-8\") as f_kor, open(eng_path, \"r\", encoding=\"utf-8\") as f_eng:\n",
    "    for kor, eng in zip(f_kor, f_eng):\n",
    "        kor_clean = clean_text(kor.strip(), lang=\"ko\")\n",
    "        eng_clean = clean_text(eng.strip(), lang=\"en\")\n",
    "\n",
    "        kor_tokenized = tokenize_text(kor_clean, lang=\"ko\")\n",
    "        eng_tokenized = tokenize_text(eng_clean, lang=\"en\")\n",
    "\n",
    "        # 병렬 데이터 쌍을 (한국어, 영어) 형태로 저장\n",
    "        unique_pairs.add((kor_tokenized, eng_tokenized))\n",
    "\n",
    "# 정제된 데이터 저장 (길이 필터링 포함)\n",
    "cleaned_corpus = []\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for kor, eng in unique_pairs:\n",
    "    if len(kor.split()) <= 40 and len(eng.split()) <= 40:  # 길이 제한 적용\n",
    "        cleaned_corpus.append((kor, eng))\n",
    "        kor_corpus.append(kor)\n",
    "        eng_corpus.append(eng)\n",
    "\n",
    "print(\"✅ 데이터 정제 및 토큰화 완료!\")\n",
    "print(f\"🔹 중복 제거 후 데이터 개수: {len(cleaned_corpus)}\")\n",
    "print(\"🔹 한국어 샘플:\", kor_corpus[:3])\n",
    "print(\"🔹 영어 샘플:\", eng_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55689657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 vocab size: 38544\n",
      "영어 vocab size: 61539\n",
      "Training samples: 66658\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')   # 특수문자 필터링 X, num_words 추후 설정\n",
    "    tokenizer.fit_on_texts(corpus)  # 단어 사전 구축\n",
    " \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # 문장을 정수 시퀀스로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 패딩 추가\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# 한국어 문장 (입력) 토큰화\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "\n",
    "# 영어 문장 (출력) 토큰화\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"한국어 vocab size:\", len(enc_tokenizer.word_index) + 1)  # 패딩 포함\n",
    "print(\"영어 vocab size:\", len(dec_tokenizer.word_index) + 1)\n",
    "print(\"Training samples:\", len(enc_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc25e05",
   "metadata": {},
   "source": [
    "# Step3 : 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ee0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832bc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec904d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba57d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a85f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b369869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 1024)\n",
      "Decoder Output: (32, 61539)\n",
      "Decoder Hidden State: (32, 1024)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cb18e",
   "metadata": {},
   "source": [
    "### Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ee8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer와 Loss 정의\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # 0을 패딩으로 간주하고 마스크 생성\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask  # 마스크를 곱하여 패딩 부분을 제외한 손실 계산\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070a830",
   "metadata": {},
   "source": [
    "### train step 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd446f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tokenizer):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)  # Encoder의 출력\n",
    "        h_dec = enc_out[:, -1]  # 마지막 hidden state\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tokenizer.word_index['<start>']] * bsz, 1)  # <start> 토큰을 decoder에 전달\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):  # t: 시퀀스의 각 timestep\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)  # Decoder 실행\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)  # 손실 계산\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)  # 다음 time step으로 이동\n",
    "\n",
    "    batch_loss = loss / int(tgt.shape[1])  # 시퀀스 길이에 따른 평균 손실 계산\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables  # 학습 가능한 변수들\n",
    "    gradients = tape.gradient(loss, variables)  # 그래디언트 계산\n",
    "    optimizer.apply_gradients(zip(gradients, variables))  # 그래디언트 적용\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5a1a0",
   "metadata": {},
   "source": [
    "### 훈련 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10b7f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2084 [00:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "   OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_24]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_144992]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_291/2751730745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n\u001b[0m\u001b[1;32m     16\u001b[0m                                 \u001b[0mdec_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:    OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_24]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_144992]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))  # 데이터 인덱스 리스트\n",
    "    random.shuffle(idx_list)  # 데이터 셔플\n",
    "    t = tqdm(idx_list)  # 진행 표시줄\n",
    "\n",
    "    for batch, idx in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)  # 훈련 단계 실행\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # tqdm 업데이트\n",
    "        t.set_description(f'Epoch {epoch + 1}')  # 에폭 표시\n",
    "        t.set_postfix(loss=f'{total_loss.numpy() / (batch + 1):.4f}')  # 현재 에폭에 대한 손실 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d22317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 후 예문 번역\n",
    "    print(\"=== Translation for Test Sentences ===\")\n",
    "    test_sentences = [\"오바마는 대통령이다.\", \"시민들은 도시 속에 산다.\", \"커피는 필요 없다.\", \"일곱 명의 사망자가 발생했다.\"]\n",
    "    for sentence in test_sentences:\n",
    "        translate(sentence, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36062cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 및 Attention 시각화\n",
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((max_dec_len, max_enc_len))  # max_dec_len과 max_enc_len 정의 필요\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=max_enc_len,\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_dec_len):  # max_dec_len 정의 필요\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "# 번역 예시 실행\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d17b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6a3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8e94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
