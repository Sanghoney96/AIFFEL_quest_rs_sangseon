{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEsc6qEiZC7n"
   },
   "source": [
    "# 한영 번역기 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNZhkKOTbSAU"
   },
   "source": [
    "### step1 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I6zuCWm0aT_u"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 다운로드\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    fname=\"korean-english-park.train.tar.gz\",\n",
    "    origin=\"https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F9SDdQCObXyO"
   },
   "outputs": [],
   "source": [
    "# 압축 파일을 풀기 위한 코드\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "extract_path = os.path.splitext(path_to_file)[0]\n",
    "\n",
    "with tarfile.open(path_to_file, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "# 파일 경로 설정\n",
    "ko_file = os.path.join(extract_path, \"korean-english-park.train.ko\")\n",
    "en_file = os.path.join(extract_path, \"korean-english-park.train.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z0C2J3TCbZS-"
   },
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def load_data(ko_path, en_path, num_samples=30000):\n",
    "    with open(ko_path, \"r\", encoding=\"utf-8\") as f_ko, open(en_path, \"r\", encoding=\"utf-8\") as f_en:\n",
    "        ko_sentences = f_ko.readlines()\n",
    "        en_sentences = f_en.readlines()\n",
    "\n",
    "    # 데이터 샘플 개수 제한\n",
    "    ko_sentences = [line.strip() for line in ko_sentences[:num_samples]]\n",
    "    en_sentences = [line.strip() for line in en_sentences[:num_samples]]\n",
    "\n",
    "    return ko_sentences, en_sentences\n",
    "\n",
    "ko_sentences, en_sentences = load_data(ko_file, en_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4yLwTrSbbmt"
   },
   "source": [
    "### step2 : 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rvy_XvZ8bapl"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 데이터 정제 함수 (한국어에 맞는 정규식 추가)\n",
    "def clean_text(text, lang=\"ko\"):\n",
    "    text = text.lower().strip()  # 공백 제거 및 소문자 변환 (영어만 해당)\n",
    "\n",
    "    # 기본 특수문자 제거 (언어별 정제)\n",
    "    if lang == \"en\":\n",
    "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)  # 영어에서 한글, 숫자 등 제거\n",
    "    else:\n",
    "        text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ?.!,¿]+\", \" \", text)  # 한국어에서 영어, 숫자 등 제거\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 연속된 공백 제거\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "Fs_4GC8cbkPt",
    "outputId": "4d4bc4ac-7741-429d-f707-f280a23bd58b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fe93a500f1b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 한국어 토큰화 (MeCab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_ko\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "# 한국어 토큰화 (MeCab)\n",
    "mecab = Mecab()\n",
    "\n",
    "def tokenize_ko(sentences):\n",
    "    return [\" \".join(mecab.morphs(sentence)) for sentence in sentences]\n",
    "\n",
    "ko_cleaned = [clean_text(sent, lang=\"ko\") for sent in ko_sentences]\n",
    "en_cleaned = [clean_text(sent, lang=\"en\") for sent in en_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "PeN0WY-Bbqsd",
    "outputId": "3e37fab7-5e56-4f20-96ae-73610876a26f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-08e53ce1b574>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# <start>와 <end> 토큰을 영어 문장에 추가하고 split() 함수를 이용해 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men_cleaned_with_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<start> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" <end>\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_cleaned\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0men_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_cleaned_with_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# <start>와 <end> 토큰을 영어 문장에 추가하고 split() 함수를 이용해 토큰화\n",
    "en_cleaned_with_tokens = [\"<start> \" + sentence + \" <end>\" for sentence in en_cleaned]\n",
    "en_tokenized = [sentence.split() for sentence in en_cleaned_with_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "TI4mPGeObsWF",
    "outputId": "697f7bfa-c240-4c43-e305-ec9fde62f5b0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ko_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-491cd546a8ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munique_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mko_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ko_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터 중복 제거\n",
    "def remove_duplicates(ko_sentences, en_sentences):\n",
    "    unique_data = set(zip(ko_sentences, en_sentences))  # 중복 제거\n",
    "    return zip(*unique_data)\n",
    "\n",
    "ko_cleaned, en_tokenized = remove_duplicates(ko_cleaned, en_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "Fp79o7R_btkt",
    "outputId": "74359efb-776b-4262-b63b-d8047699dee0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ko_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9de4740d0ddd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_ko\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mko_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ko_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# 40 이하의 길이를 가지는 문장만 필터링\n",
    "def filter_data(ko_sentences, en_sentences, max_length=40):\n",
    "    filtered_ko = [sent for sent in ko_sentences if len(sent.split()) <= max_length]\n",
    "    filtered_en = [sent for sent, kor_sent in zip(en_sentences, ko_sentences) if len(kor_sent.split()) <= max_length]\n",
    "    return filtered_ko, filtered_en\n",
    "\n",
    "ko_filtered, en_filtered = filter_data(ko_cleaned, en_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "vc7e7d3tbu89",
    "outputId": "1f1e475f-7aac-45b2-d584-b0ab0d591640"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ko_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-14583ac8833d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 한국어 및 영어 텐서 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mko_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mko_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0men_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ko_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 한국어 및 영어 토큰화 함수 정의\n",
    "def tokenize_data(corpus, num_words=10000):\n",
    "    tokenizer = Tokenizer(num_words=num_words, filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# 한국어 및 영어 텐서 변환\n",
    "ko_tensor, ko_tokenizer = tokenize_data(ko_filtered, num_words=10000)\n",
    "en_tensor, en_tokenizer = tokenize_data(en_filtered, num_words=10000)\n",
    "\n",
    "# 텐서의 샘플 출력\n",
    "print(f\"Sample Korean Tensor: {ko_tensor[0]}\")\n",
    "print(f\"Sample English Tensor: {en_tensor[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9G_09-2bzKm"
   },
   "source": [
    "### step3: 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "GDRIPfeQbyPN",
    "outputId": "254a38f5-e852-4a53-dc30-97adf5b9a4b7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ko_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-996b7953c50a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 모델 컴파일\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mvocab_size_ko\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mvocab_size_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_seq2seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size_ko\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ko_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Seq2Seq 모델 설계 (Attention 포함)\n",
    "def create_seq2seq_model(vocab_size_ko, vocab_size_en, embedding_dim=256, hidden_units=512):\n",
    "    # 인코더\n",
    "    encoder_input = layers.Input(shape=(None,))\n",
    "    encoder_embedding = layers.Embedding(input_dim=vocab_size_ko, output_dim=embedding_dim)(encoder_input)\n",
    "    encoder_lstm = layers.LSTM(hidden_units, return_state=True)\n",
    "    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "    # 디코더\n",
    "    decoder_input = layers.Input(shape=(None,))\n",
    "    decoder_embedding = layers.Embedding(input_dim=vocab_size_en, output_dim=embedding_dim)(decoder_input)\n",
    "    decoder_lstm = layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    # Attention\n",
    "    attention = layers.AdditiveAttention(use_scale=True)([decoder_outputs, encoder_outputs])\n",
    "    attention_output = layers.Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "    decoder_dense = layers.Dense(vocab_size_en, activation='softmax')\n",
    "    output = decoder_dense(attention_output)\n",
    "\n",
    "    model = Model([encoder_input, decoder_input], output)\n",
    "    return model\n",
    "\n",
    "# 모델 컴파일\n",
    "vocab_size_ko = len(ko_tokenizer.word_index) + 1\n",
    "vocab_size_en = len(en_tokenizer.word_index) + 1\n",
    "model = create_seq2seq_model(vocab_size_ko, vocab_size_en)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 구조 확인\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-Z1sFjTcC-K"
   },
   "source": [
    "### step4: 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "eldA7rMdb99O",
    "outputId": "00eb2e60-992c-4a75-cc47-37b16495ed88"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ko_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-82a4f2c2a70f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mencoder_input_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mko_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ko_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# train_step 함수 정의\n",
    "@tf.function\n",
    "def train_step(model, encoder_input, decoder_input, decoder_target, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([encoder_input, decoder_input], training=True)\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_target, logits=predictions))\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# 훈련 시작\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_start in range(0, len(ko_tensor), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(ko_tensor))\n",
    "        encoder_input_batch = ko_tensor[batch_start:batch_end]\n",
    "        decoder_input_batch = en_tensor[batch_start:batch_end, :-1]  # 디코더는 끝을 제외한 시퀀스\n",
    "        decoder_target_batch = en_tensor[batch_start:batch_end, 1:]  # 목표 값은 디코더의 shifted 버전\n",
    "\n",
    "        loss = train_step(model, encoder_input_batch, decoder_input_batch, decoder_target_batch, optimizer)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {total_loss / (len(ko_tensor) // batch_size)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7VtJiXRcRgJ"
   },
   "source": [
    "### step5: 번역 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW1poF33cFdV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 번역 함수 (입력된 한국어 문장을 영어로 번역)\n",
    "def translate_sentence(sentence, model, ko_tokenizer, en_tokenizer, max_length=40):\n",
    "    # 한국어 문장 토큰화\n",
    "    sentence_seq = ko_tokenizer.texts_to_sequences([sentence])\n",
    "    sentence_seq = pad_sequences(sentence_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # 디코더 입력 초기화\n",
    "    start_token = en_tokenizer.texts_to_sequences([\"<start>\"])[0][0]\n",
    "    end_token = en_tokenizer.texts_to_sequences([\"<end>\"])[0][0]\n",
    "\n",
    "    decoder_input = np.array([[start_token]])  # 디코더는 <start> 토큰으로 시작\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # 예측 (디코더 입력과 인코더 출력을 기반으로)\n",
    "        pred = model.predict([sentence_seq, decoder_input])\n",
    "        pred_token = np.argmax(pred[0, -1, :])  # 가장 높은 확률을 가지는 단어 선택\n",
    "\n",
    "        # 종료 조건 (예측이 <end> 토큰인 경우)\n",
    "        if pred_token == end_token:\n",
    "            break\n",
    "\n",
    "        # 예측된 단어를 디코더 입력으로 추가\n",
    "        decoded_sentence += en_tokenizer.index_word[pred_token] + \" \"\n",
    "        decoder_input = np.array([[pred_token]])\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "# 예시 문장 번역\n",
    "examples = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "# 번역 결과 출력\n",
    "for example in examples:\n",
    "    translated = translate_sentence(example, model, ko_tokenizer, en_tokenizer)\n",
    "    print(f\"Input (Korean): {example}\")\n",
    "    print(f\"Translated (English): {translated}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V90g-Pdzcf_x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_map(attention_weights, input_sentence, output_sentence, num_heads=1):\n",
    "    \"\"\"\n",
    "    attention_weights: Attention weights (shape: (num_heads, input_len, output_len))\n",
    "    input_sentence: 한국어 입력 문장 (단어 순으로)\n",
    "    output_sentence: 영어 출력 문장 (단어 순으로)\n",
    "    num_heads: Attention 헤드의 수 (기본값 1)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(15, 10))\n",
    "\n",
    "    if num_heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        sns.heatmap(attention_weights[i], annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                    xticklabels=output_sentence, yticklabels=input_sentence,\n",
    "                    cbar=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Attention Head {i + 1}')\n",
    "        axes[i].set_xlabel('Output Sequence')\n",
    "        axes[i].set_ylabel('Input Sequence')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2bd5JlQckfl"
   },
   "outputs": [],
   "source": [
    "# 모델에서 Attention 출력을 함께 반환하는 예시 (모델 구조에 맞게 조정)\n",
    "def attention_model_predict(model, sentence, ko_tokenizer, en_tokenizer, max_length=40):\n",
    "    # 한국어 문장 토큰화\n",
    "    sentence_seq = ko_tokenizer.texts_to_sequences([sentence])\n",
    "    sentence_seq = pad_sequences(sentence_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # 디코더 입력 초기화\n",
    "    start_token = en_tokenizer.texts_to_sequences([\"<start>\"])[0][0]\n",
    "    end_token = en_tokenizer.texts_to_sequences([\"<end>\"])[0][0]\n",
    "\n",
    "    decoder_input = np.array([[start_token]])  # 디코더는 <start> 토큰으로 시작\n",
    "    decoded_sentence = \"\"\n",
    "    attention_weights = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # 예측 (디코더 입력과 인코더 출력을 기반으로)\n",
    "        pred, attention_map = model.predict([sentence_seq, decoder_input])\n",
    "        attention_weights.append(attention_map)\n",
    "\n",
    "        pred_token = np.argmax(pred[0, -1, :])  # 가장 높은 확률을 가지는 단어 선택\n",
    "\n",
    "        # 종료 조건 (예측이 <end> 토큰인 경우)\n",
    "        if pred_token == end_token:\n",
    "            break\n",
    "\n",
    "        # 예측된 단어를 디코더 입력으로 추가\n",
    "        decoded_sentence += en_tokenizer.index_word[pred_token] + \" \"\n",
    "        decoder_input = np.array([[pred_token]])\n",
    "\n",
    "    return decoded_sentence.strip(), np.array(attention_weights)\n",
    "\n",
    "# 예시 문장 번역 및 Attention Map 출력\n",
    "example_sentence = \"오바마는 대통령이다.\"\n",
    "translated_sentence, attention_weights = attention_model_predict(model, example_sentence, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "# Attention Map 시각화\n",
    "input_sentence = example_sentence.split()  # 한국어 입력 문장 (공백 기준으로 토큰화)\n",
    "output_sentence = translated_sentence.split()  # 번역된 출력 문장 (공백 기준으로 토큰화)\n",
    "\n",
    "plot_attention_map(attention_weights, input_sentence, output_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
