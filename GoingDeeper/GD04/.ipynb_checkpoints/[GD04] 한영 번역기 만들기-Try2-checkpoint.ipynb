{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b7aa37",
   "metadata": {},
   "source": [
    "dropout 설정해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba5a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "# 한국어를 지원하는 폰트로 변경\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b23dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bb5988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축 해제 완료!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# 압축 풀기\n",
    "tar_path = \"./data/korean-english-park.train.tar.gz\"\n",
    "extract_path = \"./data/korean-english-park/\"\n",
    "\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "print(\"압축 해제 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a688216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean-english-park.train.en', 'korean-english-park.train.ko']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3236bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 변환 완료! 'korean.txt'와 'english.txt'가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "kor_file = \"./data/korean-english-park/korean-english-park.train.ko\"\n",
    "eng_file = \"./data/korean-english-park/korean-english-park.train.en\"\n",
    "\n",
    "# 변환된 파일 저장 경로\n",
    "kor_output = \"./data/korean.txt\"\n",
    "eng_output = \"./data/english.txt\"\n",
    "\n",
    "# 한국어, 영어 문장을 각각 저장\n",
    "with open(kor_file, \"r\", encoding=\"utf-8\") as f_ko, open(eng_file, \"r\", encoding=\"utf-8\") as f_en:\n",
    "    korean_sentences = f_ko.readlines()\n",
    "    english_sentences = f_en.readlines()\n",
    "\n",
    "# 새로운 파일로 저장\n",
    "with open(kor_output, \"w\", encoding=\"utf-8\") as f_ko, open(eng_output, \"w\", encoding=\"utf-8\") as f_en:\n",
    "    f_ko.writelines(korean_sentences)\n",
    "    f_en.writelines(english_sentences)\n",
    "\n",
    "print(\"파일 변환 완료! 'korean.txt'와 'english.txt'가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fe6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2d8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"특수 문자 및 불필요한 공백 제거\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", sentence)  # 한글과 공백만 남기기\n",
    "    else:  # 영어\n",
    "        sentence = sentence.lower().strip()  # 소문자로 변환 및 공백 제거\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]\", \" \", sentence)  # 특수 문자 제거\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 불필요한 공백 제거\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5bc1f",
   "metadata": {},
   "source": [
    "# Step2 : 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff909113",
   "metadata": {},
   "source": [
    "### 정제하기 & 토큰화하기\n",
    "- 데이터는 \\t 기호를 기준으로 영어와 스페인어가 병렬 쌍\n",
    "- \\t 기호를 매개변수로 split() 함수를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2947111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "# MeCab 토크나이저 초기화\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"특수 문자 및 불필요한 공백 제거\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", sentence)  # 한글과 공백만 남기기\n",
    "    else:  # 영어\n",
    "        sentence = sentence.lower().strip()  # 소문자로 변환 및 공백 제거\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]\", \" \", sentence)  # 특수 문자 제거\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # 불필요한 공백 제거\n",
    "    return sentence\n",
    "\n",
    "def tokenize_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"토큰화 수행\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        return mecab.parse(sentence).strip()  # MeCab 토큰화\n",
    "    else:  # 영어\n",
    "        return \"<start> \" + sentence + \" <end>\"  # 시작과 끝 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a5e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 데이터 개수: 66658\n",
      "한국어 샘플: ['그러나 그 들 은 이 의 근육 섬유 를 의 근육 섬유 로 전환 시키 는 것 같 다는 것 을 발견 하 고 는 놀랐 다', '자 회담 북측 수석대표 인 김계관 북한 외무부 부상 은 자 회담 국가 들 이 참여 한 본회 의 기조연설 에서 이 같이 밝혔 다', '한편 클린턴 은 일 펜실베이니아 경선 결과 에 따라 백악관 입 성 을 향한 결과 가 달라질 수 있 음 을 시인 했 다']\n",
      "영어 샘플: ['<start> but they were surprised to find that pgc appeared to be converting type ii fast twitch fibers into type i slow twitch fibers. <end>', '<start> vice foreign minister kim kye gwan, pyongyang s chief delegate to the six party talks, apparently made this remark at a plenary session, in which six top delegates from each of the six nations delivered keynote speeches. <end>', '<start> clinton on tuesday acknowledged her white house bid is on the line in pennsylvania. <end>']\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "kor_path = \"./data/korean.txt\"\n",
    "eng_path = \"./data/english.txt\"\n",
    "\n",
    "# 중복 제거를 위한 set 활용\n",
    "unique_pairs = set()\n",
    "\n",
    "with open(kor_path, \"r\", encoding=\"utf-8\") as f_kor, open(eng_path, \"r\", encoding=\"utf-8\") as f_eng:\n",
    "    for kor, eng in zip(f_kor, f_eng):\n",
    "        kor_clean = clean_text(kor.strip(), lang=\"ko\")\n",
    "        eng_clean = clean_text(eng.strip(), lang=\"en\")\n",
    "\n",
    "        kor_tokenized = tokenize_text(kor_clean, lang=\"ko\")\n",
    "        eng_tokenized = tokenize_text(eng_clean, lang=\"en\")\n",
    "\n",
    "        # 병렬 데이터 쌍을 (한국어, 영어) 형태로 저장\n",
    "        unique_pairs.add((kor_tokenized, eng_tokenized))\n",
    "\n",
    "# 정제된 데이터 저장 (길이 필터링 포함)\n",
    "cleaned_corpus = []\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for kor, eng in unique_pairs:\n",
    "    if len(kor.split()) <= 40 and len(eng.split()) <= 40:  # 길이 제한 적용\n",
    "        cleaned_corpus.append((kor, eng))\n",
    "        kor_corpus.append(kor)\n",
    "        eng_corpus.append(eng)\n",
    "\n",
    "print(f\"중복 제거 후 데이터 개수: {len(cleaned_corpus)}\")\n",
    "print(\"한국어 샘플:\", kor_corpus[:3])\n",
    "print(\"영어 샘플:\", eng_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d1cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 샘플 1: 그러나 그 들 은 이 의 근육 섬유 를 의 근육 섬유 로 전환 시키 는 것 같 다는 것 을 발견 하 고 는 놀랐 다\n",
      "영어 샘플 1: <start> but they were surprised to find that pgc appeared to be converting type ii fast twitch fibers into type i slow twitch fibers. <end>\n",
      "\n",
      "한국어 샘플 2: 자 회담 북측 수석대표 인 김계관 북한 외무부 부상 은 자 회담 국가 들 이 참여 한 본회 의 기조연설 에서 이 같이 밝혔 다\n",
      "영어 샘플 2: <start> vice foreign minister kim kye gwan, pyongyang s chief delegate to the six party talks, apparently made this remark at a plenary session, in which six top delegates from each of the six nations delivered keynote speeches. <end>\n",
      "\n",
      "한국어 샘플 3: 한편 클린턴 은 일 펜실베이니아 경선 결과 에 따라 백악관 입 성 을 향한 결과 가 달라질 수 있 음 을 시인 했 다\n",
      "영어 샘플 3: <start> clinton on tuesday acknowledged her white house bid is on the line in pennsylvania. <end>\n",
      "\n",
      "한국어 샘플 4: 가난 한 농장 노무자 인 랄리 의 아버지 비노드 싱 은 내 딸 은 여느 아이 들 과 다를 바 가 없 다고 말 했 다\n",
      "영어 샘플 4: <start> my daughter is fine like any other child, said vinod singh, , a poor farm worker. <end>\n",
      "\n",
      "한국어 샘플 5: 그 는 또 년 전 유도 관련 서적 을 공동 저술 하 기 도 했 다\n",
      "영어 샘플 5: <start> putin co authored a judo manual several years ago. <end>\n",
      "\n",
      "한국어 샘플 6: 미 의회 투표 를 통해 철군 결의\n",
      "영어 샘플 6: <start> united nations nuclear inspectors have been given the go ahead to return to north korea to begin the process of shutting down the main nuclear reactor. <end>\n",
      "\n",
      "한국어 샘플 7: 이반코 대변인 은 코소보 국제 평화 유지 군 소속 병사 명도 부상 했 다고 밝혔 다\n",
      "영어 샘플 7: <start> twelve soldiers from nato s force in kosovo, or kfor, were also hurt, ivanko said. <end>\n",
      "\n",
      "한국어 샘플 8: 남아공 경찰 은 관련 된 학생 들 을 모두 기소 할 방침 이 다\n",
      "영어 샘플 8: <start> the students involved in the video have been identified and will be suspended, fourie said, and charges against the men will be filed with the south african police service. <end>\n",
      "\n",
      "한국어 샘플 9: 방송 은 창조 예술 부문 개 를 포함 상 수상 으로 위 에 올랐 다\n",
      "영어 샘플 9: <start> pbs was second with awards, including creative arts trophies. <end>\n",
      "\n",
      "한국어 샘플 10: 미국 대북 경제 제재 강화 할지 도\n",
      "영어 샘플 10: <start> the industrial output growth slowed to . percent year on year last month, the lowest rate in months since june when production increased . percent, the national statistical office nso reported yesterday. <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어와 영어 문장이 일대일로 매칭되는 형태로 출력\n",
    "for i in range(10):\n",
    "    print(f\"한국어 샘플 {i+1}: {kor_corpus[i]}\")\n",
    "    print(f\"영어 샘플 {i+1}: {eng_corpus[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215c3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 vocab size: 38544\n",
      "영어 vocab size: 61539\n",
      "Training samples: 66658\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')   # 특수문자 필터링 X, num_words 추후 설정\n",
    "    tokenizer.fit_on_texts(corpus)  # 단어 사전 구축\n",
    " \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # 문장을 정수 시퀀스로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 패딩 추가\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# 한국어 문장 (입력) 토큰화\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "\n",
    "# 영어 문장 (출력) 토큰화\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"한국어 vocab size:\", len(enc_tokenizer.word_index) + 1)  # 패딩 포함\n",
    "print(\"영어 vocab size:\", len(dec_tokenizer.word_index) + 1)\n",
    "print(\"Training samples:\", len(enc_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c14329",
   "metadata": {},
   "source": [
    "# Step3 : 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70b2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52e864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0305145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dropout=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       dropout=dropout)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6321f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, dropout=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=dropout)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f59a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03456222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 512)\n",
      "Decoder Output: (32, 61539)\n",
      "Decoder Hidden State: (32, 512)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512\n",
    "embedding_dim = 216\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units, dropout = 0.3)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units, dropout = 0.3)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411c011",
   "metadata": {},
   "source": [
    "### Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer와 Loss 정의\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # 0을 패딩으로 간주하고 마스크 생성\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask  # 마스크를 곱하여 패딩 부분을 제외한 손실 계산\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c8a4f",
   "metadata": {},
   "source": [
    "### train step 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "857406a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tokenizer):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)  # Encoder의 출력\n",
    "        h_dec = enc_out[:, -1]  # 마지막 hidden state\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tokenizer.word_index['<start>']] * bsz, 1)  # <start> 토큰을 decoder에 전달\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):  # t: 시퀀스의 각 timestep\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)  # Decoder 실행\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)  # 손실 계산\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)  # 다음 time step으로 이동\n",
    "\n",
    "    batch_loss = loss / int(tgt.shape[1])  # 시퀀스 길이에 따른 평균 손실 계산\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables  # 학습 가능한 변수들\n",
    "    gradients = tape.gradient(loss, variables)  # 그래디언트 계산\n",
    "    optimizer.apply_gradients(zip(gradients, variables))  # 그래디언트 적용\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e15fb",
   "metadata": {},
   "source": [
    "### 훈련 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2084 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))  # 데이터 인덱스 리스트\n",
    "    random.shuffle(idx_list)  # 데이터 셔플\n",
    "    t = tqdm(idx_list)  # 진행 표시줄\n",
    "\n",
    "    for batch, idx in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)  # 훈련 단계 실행\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # tqdm 업데이트\n",
    "        t.set_description(f'Epoch {epoch + 1}')  # 에폭 표시\n",
    "        t.set_postfix(loss=f'{total_loss.numpy() / (batch + 1):.4f}')  # 현재 에폭에 대한 손실 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7356a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 및 Attention 시각화\n",
    "max_enc_len = 30  \n",
    "max_dec_len = 30  \n",
    "\n",
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((max_dec_len, max_enc_len))  \n",
    "    \n",
    "    sentence = clean_text(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=max_enc_len,\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_dec_len):  \n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c580fe",
   "metadata": {},
   "source": [
    "### 해결\n",
    "1. oom 해결\n",
    "    - BATCH_SIZE     = 32  \n",
    "    - + units = 512,  embedding_dim = 216  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2bf3f",
   "metadata": {},
   "source": [
    "### 이슈\n",
    "    \n",
    "1. 훈련 이후 번역을 실제로 시켜보니. the 만 반복됨.\n",
    "    - length penalty 적용해서 너무 짧은 문장이 과도하게 선택되는 것을 방지  \n",
    "    - repetition penalty 적용해서 반복 단어 방지  \n",
    "    - ... 그외는 잘 모르겠음  \n",
    "    \n",
    "    \n",
    "2. 데이터를 확인해보니 번역 범위? 가 다른 데이터들이 존재.  \n",
    "    - 이것들을 어떤 공통된 기준을 가지고 정제하기 어려움  \n",
    "      \n",
    "+2~3개씩 묶어서 하나의 인덱스로 바꿔서 augmentation 하면 성능이 좋아지지 않을까 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d7508",
   "metadata": {},
   "source": [
    "### 회고\n",
    "- 생각해야 될 것들이 너무 많지만 재밌다.\n",
    "- 아직까지는 영향을 끼치는 부분에 대해서 구체적으로 정리가 되어있지 않다고 생각한다. 이런 것들을 정리하는 시간을 가져야 겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5188a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
